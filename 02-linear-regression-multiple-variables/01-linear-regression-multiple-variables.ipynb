{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6485b3b3",
   "metadata": {},
   "source": [
    "# Multiple Variable Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9a6305",
   "metadata": {},
   "source": [
    "## Goals\n",
    "\n",
    "- Extend our regression model routines to support multiple features\n",
    "- Extend data structures to support multiple features\n",
    "- Rewrite prediction, Python functions to compute cost function and its gradient to support multiple features\n",
    "- Utilize NumPy function `numpy.dot()` or the ndarray object method `ndarray.dot()` to vectorize the implementation for speed and simplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49be49fc",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "In this lab, we will make use of\n",
    "- NumPy array, a popular library for scientific computing\n",
    "- Matplotlib, a popular library for plotting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0375b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb5dbae",
   "metadata": {},
   "source": [
    "## Notations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18766832",
   "metadata": {},
   "source": [
    "The linear regression model is formulated as\n",
    "$$f_{w, b}(x) = w x + b$$\n",
    "\n",
    "Following is a summary of notation you will encounter\n",
    "\n",
    "General notation | Description | Python (if applicable \n",
    "-----------------| ------------| --------------------- \n",
    "$a$ | scalar, non-bold |\n",
    "$\\mathbf{a}$ | vector, bold |\n",
    "$\\mathbf{x}$ | training examlpe feature values  | `x_train`\n",
    "$\\mathbf{y}$ | training example targets         | `y_train`\n",
    "$x^{(i)}, y^{(i)}$    | $i^\\mathrm{th}$ training example | `x_i`, `y_i`\n",
    "$m$          | number of training examples      | `m`\n",
    "$$\\mathbf{w}= (w_{1}, \\ldots, w_{n})$$ | parameter: weight | `w`\n",
    "$b$          | parameter: bias   | `b`\n",
    "$f_{\\mathbf{w},b}(x^{(i)})$ | The result of the model evaluation at $x^{(i)}$ parameterized by the weight $\\mathbf{w}$ and the bias $b$ | `f_wb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cac5432",
   "metadata": {},
   "source": [
    "# 2 Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6e6527",
   "metadata": {},
   "source": [
    "You will use the motivating example of house price prediction. The training dataset contains three examples with four features (size, bedrooms, floors and age) shown in the table below. Note that we use square foot instead of square meter for the area. This causes an issue, which we will try to resolve in the next *self-study notebook*.\n",
    "\n",
    "| Size (sqft) | Number of Bedrooms  | Number of floors | Age of  Home | Price (1000 Â£)  |   \n",
    "| ----------------| ------------------- |----------------- |--------------|-------------- |  \n",
    "| 600            | 2                   | 1                | 45           | 150           |  \n",
    "| 480            | 1                   | 1                | 40           | 110           |  \n",
    "| 750             | 3                   | 2                | 35           | 190           | \n",
    "\n",
    "You will build a linear regression model using these values so you can then predict the price for other houses. For example, a house with 550 sqft, 2 bedrooms, 1 floor and 37 years old."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4be047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[600, 2, 1, 45], [480, 1, 1, 40], [750, 3, 2, 35]])\n",
    "y_train = np.array([150, 110, 190])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438ff483",
   "metadata": {},
   "source": [
    "## 2.0 Solution using scikit-learn\n",
    "\n",
    "Before doing anything, let us try to obtain the parameter solution using scikit-learn. The results will be used as reference parameters for comparison in the following implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6673b201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_ref = [ 0.30769626  0.01179659 -0.03692984  0.61293052]\n",
      "b_ref = -62.186290858870166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([150., 110., 190.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "w_ref = linear_model.coef_\n",
    "b_ref = linear_model.intercept_\n",
    "print(f\"w_ref = {w_ref}\")\n",
    "print(f\"b_ref = {b_ref}\")\n",
    "\n",
    "linear_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031710dc",
   "metadata": {},
   "source": [
    "## 2.1 Matrix X containing our examples\n",
    "Similar to the table above, examples are stored in a NumPy matrix `X_train`. Each row of the matrix represents one example. When you have $m$ training examples ($m = 3$ in our example), and there are $n$ features ($n = 4$ our example), $\\mathbf{X}$ is a matrix with dimensions ($m$, $n$) (m rows, n columns).\n",
    "\n",
    "\n",
    "$$\\mathbf{X} = \n",
    "\\begin{pmatrix}\n",
    " x^{(0)}_0 & x^{(0)}_1 & \\cdots & x^{(0)}_{n-1} \\\\ \n",
    " x^{(1)}_0 & x^{(1)}_1 & \\cdots & x^{(1)}_{n-1} \\\\\n",
    " \\cdots \\\\\n",
    " x^{(m-1)}_0 & x^{(m-1)}_1 & \\cdots & x^{(m-1)}_{n-1} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "notation:\n",
    "- $\\mathbf{x}^{(i)}$ is vector containing example $i$. $\\mathbf{x}^{(i)}$ $ = (x^{(i)}_0, x^{(i)}_1, \\cdots,x^{(i)}_{n-1})$\n",
    "- $x^{(i)}_j$ is element $j$ in example $i$. The superscript in parenthesis indicates the example number while the subscript represents an element.  \n",
    "\n",
    "Display the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d2cfb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = array([[600,   2,   1,  45],\n",
      "       [480,   1,   1,  40],\n",
      "       [750,   3,   2,  35]])\n",
      "y = array([150, 110, 190])\n"
     ]
    }
   ],
   "source": [
    "# data is stored in numpy array (matrix)\n",
    "print(\"X =\", repr(X_train))\n",
    "(m, n) = X_train.shape  # well, we don't even need to store this info into m, and n -- it is obvious.\n",
    "print(\"y =\", repr(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1780d8",
   "metadata": {},
   "source": [
    "## 2.2 Parameter vector w, b\n",
    "\n",
    "* $\\mathbf{w}$ is a vector with $n$ elements.\n",
    "  - Each element contains the parameter associated with one feature.\n",
    "  - in our dataset, n is 4.\n",
    "  - notionally, we draw this as a column vector\n",
    "\n",
    "$$\\mathbf{w} = \\begin{bmatrix}\n",
    "w_0 \\\\ \n",
    "w_1 \\\\\n",
    "\\cdots\\\\\n",
    "w_{n-1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "* $b$ is a scalar parameter.\n",
    "\n",
    "For demonstration, $\\mathbf{w}$ and $b$ will be loaded with some initial selected values that are near the optimal. $\\mathbf{w}$ is a 1-D NumPy vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06e1ff33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_init shape: (4,)\n",
      "b_init type: <class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "b_init = b_ref\n",
    "w_init = w_ref\n",
    "\n",
    "print(f\"w_init shape: {w_init.shape}\")\n",
    "print(f\"b_init type: {type(b_init)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f5ff8a",
   "metadata": {},
   "source": [
    "# 3 Model Prediction With Multiple Variables\n",
    "The model's prediction with multiple variables is given by the linear model:\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
    "or in vector notation:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$ \n",
    "where $\\cdot$ is a vector `dot product`\n",
    "\n",
    "To demonstrate the dot product, we will implement prediction using (1) and (2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e687d6",
   "metadata": {},
   "source": [
    "## 3.1 Compute single prediction (element by element) using loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ebd4a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_loop(x, w, b):\n",
    "    n = x.shape[0]   # number of features\n",
    "    s = 0\n",
    "    for i in range(n):\n",
    "        s += x[i] * w[i]\n",
    "    s += b\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6e6aaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_vec shape: (4,)\n",
      "prediction #0 = 150.0\n",
      "prediction #1 = 109.99999999999994\n"
     ]
    }
   ],
   "source": [
    "# Make prediction for one training example\n",
    "x_vec = X_train[0,:]    # take the first element in the training example.\n",
    "# Note that x_vec is 1D numpy array\n",
    "print(f\"x_vec shape: {x_vec.shape}\")\n",
    "\n",
    "f_wb = predict_with_loop(x_vec, w_init, b_init)\n",
    "print(f\"prediction #0 = {f_wb}\")\n",
    "# Of course we can also write\n",
    "f_wb = predict_with_loop(X_train[1,:], w_init, b_init)\n",
    "print(f\"prediction #1 = {f_wb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0860f0f",
   "metadata": {},
   "source": [
    "## 3.2 Prediction using `numpy.dot()`\n",
    "\n",
    "The above implementation is inefficient in terms of running time and usage convenience. We can make use of vector operations to speed up predictions. Note that the sum given by\n",
    "$$s = \\sum\\limits_{j=1}^{n}x_{j}^{(i)} x_{j}^{(i)} w_j$$\n",
    "for one training example is nothing else but the dot product between two vectors \n",
    "$$\n",
    "\\mathbf{x}^{(i)} = (x_{1}^{(i)},\\ldots x_{n}^{(i)}), \\quad \\text{and} \\quad \\mathbf{w} = (w_1, \\ldots, w_n).\n",
    "$$\n",
    "That is, we can write\n",
    "$$\n",
    "f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} = \\mathbf{x}^{(i)} \\cdot \\mathbf{w}\n",
    "$$\n",
    "\n",
    "Thus, we can rewrite the above Python function with a much short piece of code. It is so short that it does not even make sense to write down the function. But we will do it anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bffa9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b):\n",
    "    return np.dot(x, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cec60757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction #0 = 150.0\n",
      "prediction #1 = 109.99999999999997\n",
      "prediction #2 = 189.99999999999997\n"
     ]
    }
   ],
   "source": [
    "# Let us repeat computing predictions as above (but now for all three training examples)\n",
    "m = X_train.shape[0]\n",
    "for i in range(m):\n",
    "    f_wb = predict(X_train[i,:], w_init, b_init)\n",
    "    print(f\"prediction #{i} = {f_wb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d042138f",
   "metadata": {},
   "source": [
    "**Power of NumPy array**\n",
    "\n",
    "The results and shapes are the same as the previous version which used looping. Going forward, `np.dot` will be used for these operations. The prediction is now a single statement. Most routines will implement it directly rather than calling a separate predict routine.\n",
    "\n",
    "What is even more interesting is that the above line of code `np.dot(x, w) + b` work not only for one example after one example. It actually works for a list of training example. What you need to do is simple. Instead of using one particular example with `X_train[i,:]`, we can use the whole matrix `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcd9c159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[150. 110. 190.]\n"
     ]
    }
   ],
   "source": [
    "print(predict(X_train, w_init, b_init))  # you can see they give the same results as above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6718cd7",
   "metadata": {},
   "source": [
    "The explanation for this is simple. Each training example is represented by a row in the matrix $\\mathbf{X}$. Thus, the dot product $\\mathbf{x}^{(i)} \\cdot \\mathbf{w}$ is actually the dot product of the row $i^\\mathrm{th}$ in the matrix $\\mathbf{X}$ and the parameter vector $\\mathbf{w}$. The matrix multiplication $\\mathbf{X}\\cdot \\mathbf{w}$ is nothing but the dot products of all of the row vectors in $\\mathbf{X}$ and $\\mathbf{w}$. Thus, the dot products between multiple training examples $\\mathbf{x}^{(i)}$ and one vector $\\mathbf{w}$ can be effectively combined into one dot product between the whole 2D array $\\mathbf{X}$ and the vector $\\mathbf{w}$.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{X}\\cdot \\mathbf{w} =\\begin{pmatrix}\n",
    " x^{(0)}_0 & x^{(0)}_1 & \\cdots & x^{(0)}_{n-1} \\\\ \n",
    " x^{(1)}_0 & x^{(1)}_1 & \\cdots & x^{(1)}_{n-1} \\\\\n",
    " \\vdots & \\ddots & \\ddots & \\vdots\\\\\n",
    " x^{(m-1)}_0 & x^{(m-1)}_1 & \\cdots & x^{(m-1)}_{n-1} \n",
    "\\end{pmatrix}\\cdot \\begin{pmatrix}\n",
    "w_0 \\\\ \n",
    "w_1 \\\\\n",
    "\\vdots\\\\\n",
    "w_{n-1}\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "\\mathbf{x}^{(0)} \\\\ \\mathbf{x}^{(1)} \\\\ \\vdots \\\\ \\mathbf{x}^{(m-1)}\n",
    "\\end{pmatrix} \\cdot \\mathbf{w} \n",
    "= \\begin{pmatrix}\n",
    "\\mathbf{x}^{(0)} \\cdot \\mathbf{w} \\\\\n",
    "\\mathbf{x}^{(1)} \\cdot \\mathbf{w} \\\\\n",
    "\\vdots\\\\\n",
    "\\mathbf{x}^{(m-1)} \\cdot \\mathbf{w}\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f0aea2",
   "metadata": {},
   "source": [
    "If you are still unsure what's going on, the above dot product between 2D array and a vector is nothing but the matrix multiplication between $\\mathbf{X}$ and a column vector given by rearrange the 1D \"mathematical\" vector $\\mathbf{w}$. To do this, can make `w` into a 2D array with one column and $n$ rows using `w.reshape((-1, 1))`. The value `-1` means the number of rows is determined appropriately after using `1` column for the `reshape` operator. See the code below for better understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e09ffd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_init = [ 0.30769626  0.01179659 -0.03692984  0.61293052]\n",
      "w_col = [[ 0.30769626]\n",
      " [ 0.01179659]\n",
      " [-0.03692984]\n",
      " [ 0.61293052]]\n",
      "w_col shape: (4, 1)\n",
      "============================================================\n",
      "X_train-dot-w_col = [[212.18629086]\n",
      " [172.18629086]\n",
      " [252.18629086]]\n",
      "X_train-dot-w = [212.18629086 172.18629086 252.18629086]\n"
     ]
    }
   ],
   "source": [
    "w_column = w_init.reshape((-1, 1))\n",
    "print(\"w_init =\", w_init)\n",
    "print(\"w_col =\", w_column)\n",
    "print(f\"w_col shape: {w_column.shape}\")\n",
    "print(\"=\"*60)\n",
    "print(\"X_train-dot-w_col =\", X_train.dot(w_column))     # return a column vector as a 2D array\n",
    "print(\"X_train-dot-w =\", X_train.dot(w_init))           # return a 1D \"mathematical\" vector, same values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2797491",
   "metadata": {},
   "source": [
    "# 4 Compute Cost with Multiple variables\n",
    "\n",
    "The equation for the cost function with multiple variables $J(\\mathbf{w},b)$ is:\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$ \n",
    "where:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{4} $$ \n",
    "\n",
    "\n",
    "In contrast to previous labs, $\\mathbf{w}$ and $\\mathbf{x}^{(i)}$ are vectors rather than scalars supporting multiple features.\n",
    "\n",
    "Below is an implementation of equations (3) and (4). Note that this uses a *standard pattern for this course* where a for loop over all `m` examples is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b70a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b): \n",
    "    \"\"\"\n",
    "    compute cost\n",
    "    Arguments:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    f_wb = np.dot(X, w) + b\n",
    "    cost = np.sum((f_wb - y) ** 2) / (2 * m)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cbe6dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at initial parameters = 6.731613057885968e-28\n"
     ]
    }
   ],
   "source": [
    "# Compute and display cost using the reference parameter\n",
    "# solution obtained from scikit-learn\n",
    "cost = compute_cost(X_train, y_train, w_ref, b_ref)\n",
    "print(f\"Cost at initial parameters = {cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f397a735",
   "metadata": {},
   "source": [
    "# 5 Gradient Descent with Multiple Variables\n",
    "\n",
    "Gradient descent for multiple variables:\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
    "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{for j = 0..n-1}\\newline\n",
    "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{7}\n",
    "\\end{align}\n",
    "$$\n",
    "* m is the number of training examples in the data set\n",
    "\n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2a15e8",
   "metadata": {},
   "source": [
    "## 5.1 Compute Gradient with Multiple Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5fcbaa",
   "metadata": {},
   "source": [
    "### 5.1.1. Option 1\n",
    "\n",
    "An implementation for calculating the equations (6) and (7) is below. There are many ways to implement this. In this version, there is an\n",
    "- outer loop over all m examples. \n",
    "    - $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ for the example can be computed directly and accumulated\n",
    "    - in a second loop over all n features:\n",
    "        - $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}$ is computed for each $w_j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c882ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_with_two_loop(X, y, w, b): \n",
    "    \"\"\"See the doc-string in the next function.\"\"\"\n",
    "    m,n = X.shape           #(number of examples, number of features)\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):                             \n",
    "        err = (np.dot(X[i], w) + b) - y[i]   \n",
    "        for j in range(n):                         \n",
    "            dj_dw[j] = dj_dw[j] + err * X[i, j]    \n",
    "        dj_db = dj_db + err                        \n",
    "    dj_dw = dj_dw / m                                \n",
    "    dj_db = dj_db / m                                \n",
    "        \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce34a44c",
   "metadata": {},
   "source": [
    "As in the last Week, it is always wise to double check the implementation of gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5aea9e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finite difference dJ_dw = [205370.99999987    698.20000008    458.46666664  12952.33333338]\n",
      "                  dJ_dw = [205371.            698.2           458.46666667  12952.33333333]\n"
     ]
    }
   ],
   "source": [
    "b_test = b_ref + 0.1\n",
    "w_test = w_ref + 0.5\n",
    "def test(func_grad):\n",
    "    epsilon = 1e-4\n",
    "    n = X_train.shape[1]\n",
    "    dJ_dw_approx = []\n",
    "    for i in range(n):\n",
    "        w1 = w_test.copy(); w1[i] -= epsilon\n",
    "        w2 = w_test.copy(); w2[i] += epsilon\n",
    "        J1 = compute_cost(X_train, y_train, w1, b_test)\n",
    "        J2 = compute_cost(X_train, y_train, w2, b_test)\n",
    "        dJ_dw_approx.append((J2 - J1) / (2*epsilon))\n",
    "\n",
    "    dJ_dw_fd = np.array(dJ_dw_approx)  # fd stands for finite difference\n",
    "    \n",
    "    # use specific function that is passed to test()\n",
    "    dJ_dw, dJ_db = func_grad(X_train, y_train, w_test, b_test)  \n",
    "    \n",
    "    print(f\"finite difference dJ_dw = {dJ_dw_fd}\")\n",
    "    print(f\"                  dJ_dw = {dJ_dw}\")\n",
    "\n",
    "test(compute_gradient_with_two_loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194ce894",
   "metadata": {},
   "source": [
    "### 5.1.2 Option 2\n",
    "\n",
    "It turns out that we can aliviate one loop in the implementation of computing the cost gradient. Indeed, the loop over all the features, namely `for j in range(n)` is not needed. We can see that the index `j` apprears on both sides of the expression\n",
    "```Python\n",
    "dj_dw[j] = dj_dw[j] + err * X[i, j]\n",
    "```\n",
    "This shows the sign that we can replace this line code and remove the `for` loop over the running index `j` by vectorization concept. Indeed, the whole loop\n",
    "```Python\n",
    "for j in range(n):                         \n",
    "            dj_dw[j] = dj_dw[j] + err * X[i, j]\n",
    "```\n",
    "is equivalent to\n",
    "```Python\n",
    "dj_dw = dj_dw + err * X[i,:]\n",
    "```\n",
    "\n",
    "Keepind this in mind, we can rewrite the function `compute_gradient_with_loop` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4684d0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_with_single_loop(X, y, w, b): \n",
    "    \"\"\"See the doc-string in the next function definition\"\"\"\n",
    "    m,n = X.shape           #(number of examples, number of features)\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):                             \n",
    "        err = (np.dot(X[i], w) + b) - y[i]   \n",
    "        dj_dw = dj_dw + err * X[i,:]   # no need to write for-loop\n",
    "        dj_db = dj_db + err                        \n",
    "    dj_dw = dj_dw / m                                \n",
    "    dj_db = dj_db / m                                \n",
    "        \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95782074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finite difference dJ_dw = [205370.99999987    698.20000008    458.46666664  12952.33333338]\n",
      "                  dJ_dw = [205371.            698.2           458.46666667  12952.33333333]\n"
     ]
    }
   ],
   "source": [
    "test(compute_gradient_with_single_loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7d7259",
   "metadata": {},
   "source": [
    "### 5.1.3 Option 3\n",
    "\n",
    "It is probably natural to ask whether we can completely remove the last `for` loop. Answer is **yes**. Just think about the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56952ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b):\n",
    "    \"\"\"Compute the gradient for linear regression \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    f_wb = np.dot(X, w) + b\n",
    "    err = f_wb - y\n",
    "    dj_dw = np.dot(err, X) / m\n",
    "    dj_db = np.sum(err) / m\n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "093d921d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finite difference dJ_dw = [205370.99999987    698.20000008    458.46666664  12952.33333338]\n",
      "                  dJ_dw = [205371.            698.2           458.46666667  12952.33333333]\n"
     ]
    }
   ],
   "source": [
    "test(compute_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8cac13",
   "metadata": {},
   "source": [
    "## 5.2 Gradient Descent with Multiple Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c328c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "402d625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, func_value, func_grad, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Perform batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters  \n",
    "      b_in (scalar)       : initial model parameter\n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters \n",
    "      b (scalar)       : Updated value of parameter \n",
    "      \"\"\"\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_dw, dj_db = func_grad(X, y, w, b)   ##None\n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               ##None\n",
    "        b = b - alpha * dj_db               ##None\n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion\n",
    "            J_history.append(func_value(X, y, w, b))\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% np.ceil(num_iters / 10) == 0:\n",
    "            print(\"Iteration {0:4d}: Cost {1:8.2f}\".format(i, J_history[-1]))\n",
    "        \n",
    "    return w, b, J_history #return final w,b and J history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4a7bb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.0\n",
    "# some gradient descent settings\n",
    "iterations = 1000000\n",
    "alpha = 5.0e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d4423a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 10174.61\n",
      "Iteration 100000: Cost     6.74\n",
      "Iteration 200000: Cost     6.74\n",
      "Iteration 300000: Cost     6.74\n",
      "Iteration 400000: Cost     6.74\n",
      "Iteration 500000: Cost     6.74\n",
      "Iteration 600000: Cost     6.74\n",
      "Iteration 700000: Cost     6.74\n",
      "Iteration 800000: Cost     6.74\n",
      "Iteration 900000: Cost     6.74\n",
      "\n",
      "b,w found by gradient descent:\n",
      "============================================================\n",
      "b_final = -0.9946625257155384\n",
      "w_final = [ 0.2639255   3.2868849  -2.19745429 -0.33688748]\n",
      "============================================================\n",
      "w_ref = [ 0.30769626  0.01179659 -0.03692984  0.61293052]\n",
      "b_ref = -62.186290858870166\n",
      "============================================================\n",
      "prediction: 146.58 -- target value: 150\n",
      "prediction: 113.30 -- target value: 110\n",
      "prediction: 190.62 -- target value: 190\n"
     ]
    }
   ],
   "source": [
    "# run gradient descent \n",
    "w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b, \\\n",
    "                                            compute_cost, compute_gradient, alpha, iterations)\n",
    "print(f\"\\nb,w found by gradient descent:\\n\" + \"=\"*60)\n",
    "print(f\"b_final = {b_final}\")\n",
    "print(f\"w_final = {w_final}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"w_ref = {w_ref}\")\n",
    "print(f\"b_ref = {b_ref}\")\n",
    "print(\"=\"*60)\n",
    "m,_ = X_train.shape\n",
    "for i in range(m):\n",
    "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f} -- target value: {y_train[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ac17fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([150., 110., 190.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a428333",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "You can see that the result of the gradient descent and the Linear Regression model from sklearn do not agree with each other well. This is because the gradient descent has not converged well enough. We will learn that the problem is that the values of different features are in different range and this makes the gradient descent struggle to converge. To overcome this issue, we will learn about feature normalization in the next lecture to fix this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01fb650",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
