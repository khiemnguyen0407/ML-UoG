{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"\">**Introduction to PyTorch**</font>  \n",
    "<font color=\"\">**Tensors in PyTorch**</font>  \n",
    "<u>**Lecturer**</u>: <font color=\"DeepSkyBlue\">*Khiem Nguyen*</font> -- *James Watt School of Engeering - University of Glasgow*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "\n",
    "**Note** \n",
    "\n",
    "This notebook is inspired by the tutorial series [Tutorial on Pytorch](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html). However, if you read along, you will find there are more detailed explanation and useful information. The online tutorial assumes that you understand NumPy and Python very well, thus provides not so much explanation.\n",
    "\n",
    "## Background\n",
    "\n",
    "PyTorch is a machine learning library based on the Torch library, used for applications such as computer vision and natural language processing, originally developed by Meta AI and now part of the Linux Foundation umbrella. It is one of the most popular deep learining frameworks, alongside others such as TensorFlow, offering free and open-source software. As the name prefix **Py** suggests, PyTorch has Python interface which is the primary focus of development. However, PyTorch also has a C++ interface and is mostly developed in the C++ langage base. for performance purposes.\n",
    "\n",
    "Meta (Facebook in the past) operates both PyTorch. In September 2022, Meta announced that PyTroch would be governed by the independent PyTorch Foundation, a newly created subsididary of the Linux Foundation. This is apparently a good news as Linux is unarguably the most widely used operation system in many electrical devices. In fact, PyTorch is unarguably the most widely used machine learning framework at the point of writing this Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "In simplified langauge, tensors are a specialized data structure that are very similar to arrays and matrices. A tensor is a mathematical tool which is widely used in various research areas of physics and engineering. If you learn, for example, general relativity, quantum mechanics, fluid mechanics, solid mechanics, you will definitely learn about Tensors. As it is widely used in various branches of research, it is not a surprise you see it in machine learning too. In mathematics, a tensor is an algebraic object that describes a multi-linear relationship betwen sets of algebraic objects related to a vector space. For example, a matrix is just a representation of a mapping from one multi-dimensional space to another multi-dimensional space. In a multi-dimensional space, a tensor can be represented by a numerical array. Of course, one cannot learn a complex mathematical concept by just reading Wikpedia, but here it is for your own interest: [Wikipedia Link](https://en.wikipedia.org/wiki/Tensor)\n",
    "\n",
    "**Important Remark**\n",
    "\n",
    "Although it is a complex mathematical concept, it is safe for us in this course and many engineering courses to view a tensor as a multi-dimensional array just like an array in *NumPy*. In this notebook, we assume that you are somewhat familiar with ndarrays defined in NumPy. You will see that tensors and ndarrays are very similar but not identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor initialization\n",
    "\n",
    "We define a tensor just like we define a NumPy array. We can also create tensors from NumPy arrays by converting an ndarray into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2], [3, 4]]         \n",
    "x_data = torch.tensor(data)     # look just like x = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "np_array = np.array(data)           # create a numpy array\n",
    "x_np = torch.from_numpy(np_array)   # convert the numpy array to a tensor\n",
    "print(x_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like NumPy, we can create tensors using the same functions as in NumPy. The following are some functions you should find familiar from NumPy experience. It is best to illustrate this by examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ones_tensor =\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "zeros_tensor =\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "rand_tensor =\n",
      "tensor([[0.3206, 0.5622, 0.9485],\n",
      "        [0.8694, 0.5978, 0.2762]])\n",
      "\n",
      "two_tensor =\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "ones_tensor = torch.ones(size=(2, 3))       # Note the keyword argument \"size\", its counterpart in NumPy is \"shape\"\n",
    "zeros_tensor = torch.zeros((2, 3))          # We don't have to use keyword argument though.\n",
    "rand_tensor = torch.rand(size=(2, 3))\n",
    "\n",
    "# You can also define the shape as tuple with the last empty element\n",
    "shape = (2, 3,)\n",
    "two_tensor = 2 * torch.ones(size=shape)     # the multiplication is element-wise operated\n",
    "\n",
    "print(f\"ones_tensor =\\n{ones_tensor}\\n\")\n",
    "print(f\"zeros_tensor =\\n{zeros_tensor}\\n\")\n",
    "print(f\"rand_tensor =\\n{rand_tensor}\\n\")\n",
    "print(f\"two_tensor =\\n{two_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in NumPy, we have `torch.ones_like(data)`, `torch.zeros_like(data)` and `torch.rand_like(data)` in Torch.\n",
    "- `torch.ones_like()` create a tensor of all elements with value $1$ with the shape/size taken from the shape of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =\n",
      "tensor([[1, 2, 4],\n",
      "        [4, 5, 6]], dtype=torch.int16)\n",
      "\n",
      "x_rand =\n",
      "tensor([[0.7990, 0.4156, 0.7186],\n",
      "        [0.8311, 0.4549, 0.4889]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 4], [4.4, 5.6, 6.5]], dtype=torch.int16)     # You can specify the data type too.\n",
    "x_ones = torch.ones_like(x)\n",
    "\n",
    "# if you don't define the dtype, it will give error as it will infer from \n",
    "# the data type of x as integer. However, is not possible for a number \n",
    "# between 0 and 1 to be an integer.\n",
    "x_rand = torch.rand_like(x, dtype=torch.float32)    \n",
    "\n",
    "print(f\"x =\\n{x}\\n\")            # the floating values are then round up to integers.\n",
    "print(f\"x_rand =\\n{x_rand}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of tensor\n",
    "\n",
    "Tensor has various attributes. Three commonly used attributes are **shape**, **datatype**, and **device** on which they are stored. We can create tensor on CPU or GPU. If the tensor is defined on CPU, the device value is `'cpu'` and if the tensor is defined on CUDA, the device value is `'cuda'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device tensor is stored on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# My local machine (laptop) has Nvidia GPU. I also install pytorch on CUDA.\n",
    "# Therefore, I can define a tensor on CUDA.\n",
    "tensor = torch.rand(size=(3, 4), device='cuda')         \n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations on Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over $100$ tensor operations, including arithmetic, linear algebra, matrix manipulation such as transposing, indexing, sclicing, sampling and more are comprehensively described [here](https://pytorch.org/docs/stable/torch.html). We will go through some of the most basic and commonly used operations. It is best to learn them by examples. For more information, it is advisable to google the syntax or to use the Python documentation with the question mark `function_name?` syntax\n",
    "\n",
    "By default, tensors are created on the CPU. We need to explicitly move tensors to the GPU using .to method (after checking for GPU availability). Keep in mind that copying large tensors across devices can be expensive in terms of time and memory!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standar numpy-like indexing and slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row: tensor([1., 1., 1., 1.])\n",
      "First column: tensor([1., 1., 1., 1.])\n",
      "Last column: tensor([1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones((4, 4))\n",
    "print(f\"First row: {tensor[0]}\")\n",
    "print(f\"First column: {tensor[:, 0]}\")\n",
    "print(f\"Last column: {tensor[..., -1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Change all the values in the second column to 0\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Joining tensors** &nbsp; We can use `torch.cat()` to concatenate a sequence of tensors along a given dimension. Another joining operator that is subtly different from `torch.cat()` is `torch.stack()`. It is always good to have a look at the online documentation at one point in your life 😄. You can easily google them; here are two examples:\n",
    "- [torch.cat](https://pytorch.org/docs/stable/generated/torch.stack.html)\n",
    "- [torch.stack](https://pytorch.org/docs/stable/generated/torch.stack.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arithmetic operations\n",
    "\n",
    "Just like NumPy, all of the standard arithmetic operations are executed in the element-wise fashion. We also have matrix multiplication between two tensors (2D tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a + b = tensor([5, 7, 9])\n",
      "a - b = tensor([-3, -3, -3])\n",
      "a * b = tensor([ 4, 10, 18])\n",
      "a / b = tensor([0.2500, 0.4000, 0.5000])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "print(f\"a + b = {a + b}\")\n",
    "print(f\"a - b = {a - b}\")\n",
    "print(f\"a * b = {a * b}\")\n",
    "print(f\"a / b = {a / b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use functions for operations just like in NumPy. Of course, it is more convenient to use the operators than to use object methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a + b = tensor([5, 7, 9])\n",
      "a - b = tensor([-3, -3, -3])\n",
      "a * b = tensor([ 4, 10, 18])\n",
      "a / b = tensor([0.2500, 0.4000, 0.5000])\n"
     ]
    }
   ],
   "source": [
    "print(f\"a + b = {a.add(b)}\")\n",
    "print(f\"a - b = {a.sub(b)}\")\n",
    "print(f\"a * b = {a.mul(b)}\")\n",
    "print(f\"a / b = {a.div(b)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected m1 and m2 to have the same dtype, but got: double != float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m]],  dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat64)  \u001b[38;5;66;03m# if you don't specify the data type, it will be inferred as integer.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma @ b =\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma @ b =\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00ma\u001b[38;5;250m \u001b[39m\u001b[38;5;241m@\u001b[39m\u001b[38;5;250m \u001b[39mb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected m1 and m2 to have the same dtype, but got: double != float"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, 3], [4, 5, 6]],  dtype=torch.float64)  # if you don't specify the data type, it will be inferred as integer.\n",
    "b = torch.ones(size=(3, 4))\n",
    "print(f\"a @ b =\\n{a.matmul(b)}\")\n",
    "print(f\"a @ b =\\n{a @ b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation\n",
    "\n",
    "I don't think I need to write \"just like NumPy\" anymore, because there are really a lot of things like NumPy. Before going to examples, let me repeat that we can perform aggregation operations by using the object methods acting on the objects themself, or by using the library functions receiving objects as input arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum(x)  = 24.0\n",
      "\n",
      "x.sum() = 24.0\n",
      "\n",
      "x.sum(dim=2) =\n",
      "tensor([[4., 4., 4.],\n",
      "        [4., 4., 4.]])\n",
      "\n",
      "torch.sum(x, dim=2) =\n",
      "tensor([[4., 4., 4.],\n",
      "        [4., 4., 4.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(size=(2, 3, 4))\n",
    "print(f\"sum(x)  = {torch.sum(x)}\\n\")\n",
    "print(f\"x.sum() = {x.sum()}\\n\")\n",
    "\n",
    "# We can perform aggregation along particular dimensions\n",
    "print(f\"x.sum(dim=2) =\\n{x.sum(dim=2)}\\n\")\n",
    "print(f\"torch.sum(x, dim=2) =\\n{torch.sum(x, dim=2)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single-element tensors** &nbsp; If we have a one-element tensor, for example, by aggregating all values of a tensor into one value (as in the above example), we can convert it to a Python numerical value using `item()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.) -- <class 'torch.Tensor'>\n",
      "6.0 -- <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(size=(2, 3))\n",
    "agg = x.sum()\n",
    "agg_item = agg.item()\n",
    "print(agg, \"--\", type(agg))\n",
    "print(agg_item, \"--\", type(agg_item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In-place operations** &nbsp; Operations that store the result into the operand are called in-place. They are denoted by a `_` suffix. For example: `x.copy_(y)`, `x.t_()`, will change `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6., 6., 6.],\n",
      "        [6., 6., 6.]])\n",
      "tensor([[6., 6., 6.],\n",
      "        [6., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(size=(2, 3))\n",
    "x.add_(5)       # this is essentially equal to x += 5\n",
    "print(x)\n",
    "\n",
    "x = torch.ones(size=(2, 3))\n",
    "x += 5\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bridge with NumPy\n",
    "\n",
    "Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor to NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_torch <class 'torch.Tensor'> =\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "\n",
      "x_numpy <class 'numpy.ndarray'> =\n",
      "array([1., 1., 1., 1., 1.], dtype=float32)\n",
      "\n",
      "Difference between repr vs str\n",
      "============================================================\n",
      "repr(x_numpy) = array([1., 1., 1., 1., 1.], dtype=float32)\n",
      "str(x_numpy) = [1. 1. 1. 1. 1.] \n",
      "\n",
      "repr(x_torch) = tensor([1., 1., 1., 1., 1.])\n",
      "str(x_torch) = tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "x_torch = torch.ones(5)     # this is a torch tensor\n",
    "\n",
    "x_numpy = x_torch.numpy()         # convert a tensor to a numpy array\n",
    "print(f\"x_torch {type(x_torch)} =\\n{repr(x_torch)}\\n\")  # repr is for developers, str is for normal users\n",
    "print(f\"x_numpy {type(x_numpy)} =\\n{repr(x_numpy)}\\n\")\n",
    "\n",
    "print(\"Difference between repr vs str\\n\" + 60*\"=\")\n",
    "print(f\"repr(x_numpy) = {repr(x_numpy)}\")\n",
    "print(f\"str(x_numpy) = {str(x_numpy)} \\n\")\n",
    "\n",
    "# You don't see the difference for tensor though\n",
    "print(f\"repr(x_torch) = {repr(x_torch)}\")\n",
    "print(f\"str(x_torch) = {str(x_torch)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A change in the tensor reflects in the NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_torch =\n",
      "tensor([2., 2., 2., 2., 2.])\n",
      "x_numpy =\n",
      "array([2., 2., 2., 2., 2.], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x_torch.add_(1)\n",
    "print(f\"x_torch =\\n{repr(x_torch)}\")\n",
    "print(f\"x_numpy =\\n{repr(x_numpy)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NumPy array to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_torch = tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n",
      "y_numpy = [3. 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "y_numpy = 2 * np.ones(5)\n",
    "y_torch = torch.from_numpy(y_numpy)\n",
    "\n",
    "# Again, a change the in NumPy array reflects in the tensor.\n",
    "np.add(y_numpy, 1, out=y_numpy)    # this is just y_numpy += 1\n",
    "\n",
    "print(f\"y_torch = {y_torch}\")\n",
    "print(f\"y_numpy = {y_numpy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark on default data type in NumPy and Torch** \n",
    "\n",
    "\n",
    "If you pay enough attention, you see that by default NumPy implies data type `np.float64` but Torch implies data type `torch.float32`. So, if we create a tensor, we have **float32** data array. Therefore, when we convert that tensor to a NumPy array, that NumPy array will be inferred with the data type **float32**. On the other hand, we create a NumPy array, we have **float64** data array and obtain the tensor of data type **float64** on the conversion from NumPy array to Torch array. This conversion normally creates trouble when we try to compute the gradient of a function when we have to convert NumPy array to Torch array and have other Torch array of data type **float32**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_numpy.dtype = float64\n",
      "x_torch.dtype = torch.float64\n"
     ]
    }
   ],
   "source": [
    "x_numpy = np.random.rand(3, 2)\n",
    "x_torch = torch.from_numpy(x_numpy)\n",
    "print(f\"x_numpy.dtype = {x_numpy.dtype}\")\n",
    "print(f\"x_torch.dtype = {x_torch.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_numpy.dtype = float32\n",
      "x_torch.dtype = torch.float32\n"
     ]
    }
   ],
   "source": [
    "x_torch = torch.rand(size=(3, 2))\n",
    "x_numpy = x_torch.numpy()\n",
    "print(f\"x_numpy.dtype = {x_numpy.dtype}\")\n",
    "print(f\"x_torch.dtype = {x_torch.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5262, 1.4955],\n",
      "        [1.4083, 1.3363],\n",
      "        [1.2626, 1.0254]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(size=(3, 2))\n",
    "y = torch.from_numpy(np.ones(shape=(3, 2)))\n",
    "print(x + y)    # it automatically converts to \"bigger\" datatype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But** things might go wrong sometimes when we try to train a neural network. We will learn more about **automatic differentiaion** in the next Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
